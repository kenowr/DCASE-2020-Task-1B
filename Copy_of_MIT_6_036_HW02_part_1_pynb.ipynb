{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of MIT 6.036 HW02 part 1.pynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kenowr/DCASE-2020-Task-1B/blob/master/Copy_of_MIT_6_036_HW02_part_1_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xIaEwCD406A"
      },
      "source": [
        "#MIT 6.036 Fall 2021: Homework 02 Part 1#\n",
        "\n",
        "**If you haven't already, please hit :**\n",
        "\n",
        "`File` -> `Save a Copy in Drive`\n",
        "\n",
        "**to copy this notebook to your Google drive, and work on a copy. If you don't do this, your changes won't be saved!**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This colab notebook provides code and a framework for problems 1-6 of the homework.  You can work out your solutions here, then submit your results back on the homework page when ready.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb4bXJC3sFT-"
      },
      "source": [
        "## <section>**Setup**</section>\n",
        "\n",
        "\n",
        "Run the next code block to download and import the code for this lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YM-_zLf9Bp-"
      },
      "source": [
        "!rm -f code_for_hw02.py*\n",
        "!wget --no-check-certificate --quiet https://introml.odl.mit.edu/cat-soop/_static/6.036/homework/hw02/code_for_hw02.py \n",
        "from code_for_hw02 import *\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bhI4dQB1-UZ"
      },
      "source": [
        "## 4) Implement Perceptron\n",
        "\n",
        "Implement [the Perceptron algorithm](https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2020_Fall/courseware/Week2/perceptron/2), where\n",
        "\n",
        "* `data` is a numpy array of dimension $d$ by $n$\n",
        "* `labels` is numpy array of dimension $1$ by $n$\n",
        "* `params` is a dictionary specifying extra parameters to this algorithm; your algorithm should run a number of iterations equal to $T$\n",
        "\n",
        "\n",
        "It should return a tuple of $\\theta$ (a $d$ by 1 array) and $\\theta_0$ (a 1 by 1 array).\n",
        "\n",
        "We have given you some  data sets in the code file for you to test your implementation. Below are some test cases.\n",
        "```\n",
        "# Test Case 1\n",
        ">>> data = np.array([[2, 3, 9, 12],\n",
        "                     [5, 2, 6, 5]])\n",
        ">>> labels = np.array([[1, -1, 1, -1]])\n",
        ">>> [x.tolist() for x in perceptron(data, labels, {\"T\": 100})]\n",
        "[[[-24.0], [37.0]], [[-3.0]]]\n",
        "\n",
        "# Test Case 2\n",
        ">>> data = np.array([[1, 2, 1, 2],\n",
        "                     [1, 2, 2, 1]])\n",
        ">>> labels = np.array([[1, 1, -1, -1]])\n",
        ">>> [x.tolist() for x in perceptron(data, labels, {\"T\": 100})]\n",
        "[[[0.0], [-3.0]], [[0.0]]]\n",
        "```\n",
        "\n",
        "Your function should initialize any parameters defined in the function to 0, then run through the data, in the order it is given, performing an update to the parameters whenever the current parameters would make a mistake on that data point. Perform `T` iterations through the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtYf8ysk-VQU"
      },
      "source": [
        "def perceptron(data, labels, params={}):\n",
        "    # if T not in params, default to 100\n",
        "    T = params.get('T', 100)\n",
        "\n",
        "    # Your implementation here\n",
        "    num_row, num_col = np.array(np.shape(data))\n",
        "    th = np.zeros((num_row,1))\n",
        "    th0 = np.zeros((1,1))\n",
        "\n",
        "    for i1 in range(T):\n",
        "      changed = False\n",
        "\n",
        "      for i2 in range(num_col):\n",
        "        if labels[0,i2] * (th.T @ data[:,i2:i2+1] + th0) <= 0.0:\n",
        "          th = th + labels[0:,i2] * data[:,i2:i2+1]\n",
        "          th0 = th0 + labels[0:,i2:i2+1]\n",
        "          changed = True\n",
        "\n",
        "      if changed == False:\n",
        "        break\n",
        "\n",
        "    return (th, th0)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92r2oL42-yfM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fd4e082-664e-4a6c-95b9-5994bf5e2527"
      },
      "source": [
        "test_perceptron(perceptron)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------Test Perceptron 0-----------\n",
            "Passed! \n",
            "\n",
            "-----------Test Perceptron 1-----------\n",
            "Passed! \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQMcSWlmB4-Y"
      },
      "source": [
        "# 5) Implement Averaged Perceptron\n",
        "\n",
        "We know that perceptron is guaranteed to find a separator if the data is linearly separable, but what can we do if the data is not linearly separable! Finding the separator that minimizes the number of mistakes could take an amount of computation that grows exponentially with the number of training examples, so we might settle for an approximate solution. Simply running the perceptron algorithm for a while, and then stopping and returning the result is not the best strategy, because the output hypothesis is highly influenced by the data points it has seen most recently. We can make a version of the perceptron that’s more stable and robust for non-separable data by returning the average value of th and th0 across all iterations. This is called the averaged perceptron algorithm.\n",
        "\n",
        "Implement averaged Perceptron with the same spec as regular Perceptron (implemented above).\n",
        "\n",
        "Below are a couple of example test cases.\n",
        "```\n",
        "# Test Case 1\n",
        ">>> data = np.array([[2, 3, 9, 12],\n",
        "                     [5, 2, 6, 5]])\n",
        ">>> labels = np.array([[1, -1, 1, -1]])\n",
        ">>> [x.tolist() for x in averaged_perceptron(data, labels, {\"T\": 100})]\n",
        "[[[-22.1925], [34.06]], [[-2.1725]]]\n",
        "\n",
        "# Test Case 2\n",
        ">>> data = np.array([[1, 2, 1, 2],\n",
        "                     [1, 2, 2, 1]])\n",
        ">>> labels = np.array([[1, 1, -1, -1]])\n",
        ">>> [x.tolist() for x in averaged_perceptron(data, labels, {\"T\": 100})]\n",
        "[[[1.47], [-1.7275]], [[0.985]]]\n",
        "```\n",
        "\n",
        "\n",
        "Implement averaged perceptron with the same spec as regular perceptron."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAwW00MU_FzS"
      },
      "source": [
        "def averaged_perceptron(data, labels, params={}):\n",
        "    # if T not in params, default to 100\n",
        "    T = params.get('T', 100)\n",
        "\n",
        "    # Your implementation here\n",
        "    num_row, num_col = np.array(np.shape(data))\n",
        "    th = np.zeros((num_row,1))\n",
        "    th0 = np.zeros((1,1))\n",
        "    th_running = np.zeros((num_row,1))\n",
        "    th0_running = np.zeros((1,1))\n",
        "    counter = 0\n",
        "\n",
        "    for i1 in range(T):\n",
        "      changed = False\n",
        "\n",
        "      for i2 in range(num_col):\n",
        "        if labels[0,i2] * (th.T @ data[:,i2:i2+1] + th0) <= 0:\n",
        "          th = th + labels[0:,i2] * data[:,i2:i2+1]\n",
        "          th0 = th0 + labels[0:,i2:i2+1]\n",
        "          changed = True\n",
        "      \n",
        "        th_running[:,0] = th_running[:,0] + th[:,0]\n",
        "        th0_running[:,0] = th0_running[:,0] + th0\n",
        "        counter = counter + 1 \n",
        "      \n",
        "      if changed == False:\n",
        "        break\n",
        "\n",
        "    return (th_running/counter, th0_running/counter)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyLGH0_cBFSU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e2d95b4-39f5-4ee5-f569-22fd6c147d94"
      },
      "source": [
        "test_averaged_perceptron(averaged_perceptron)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------Test Averaged Perceptron 0-----------\n",
            "Test Failed.\n",
            "Your code output  th: [[-9.875], [11.041666666666666]], th0: [[1.0416666666666667]]\n",
            "Expected  th: [[-9.0525], [17.5825]], th0: [[1.9425]]\n",
            "\n",
            "\n",
            "-----------Test Averaged Perceptron 1-----------\n",
            "Passed! \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXS9mYimkfKd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTfGq7LNGceQ"
      },
      "source": [
        "# 6) Implement Evaluation Strategies\n",
        "  \n",
        "## 6.1)  Evaluating a Classifier\n",
        "\n",
        "To evaluate a classifier, we are interested in how well it performs on data that it wasn't trained on. Construct a testing procedure that uses a training data set, calls a learning algorithm to get a linear separator (a tuple of $\\theta, \\theta_0$), and then reports the percentage correct on a new testing set as a float between $0.0$ and $1.0$.\n",
        "\n",
        "The learning algorithm is passed as a function that takes a data array and a labels vector.  Your evaluator should be able to interchangeably evaluate `perceptron` or `averaged_perceptron` (or future algorithms with the same spec), depending on what is passed through the `learner` parameter.\n",
        "\n",
        "Assume that you have available the function `score` from HW 1, which takes inputs:\n",
        "\n",
        "* `data`: a `d` by `n` array of floats (representing `n` data points in `d` dimensions)\n",
        "* `labels`: a `1` by `n` array of elements in `(+1, -1)`, representing target labels\n",
        "* `th`: a `d` by `1` array of floats that together with `th0`, represents a hyperplane\n",
        "* `th0`: a single scalar or `1` by `1` array\n",
        "\n",
        "and returns a scalar number of data points that the separator correctly classified.\n",
        "\n",
        "The `eval_classifier` function should accept the following parameters:\n",
        "\n",
        "* `learner` - a function, such as `perceptron` or `averaged_perceptron`\n",
        "* `data_train` - training data\n",
        "* `labels_train` - training labels\n",
        "* `data_test` - test data\n",
        "* `labels_test` - test labels\n",
        "\n",
        "and returns the percentage correct on a new testing set as a float between $0.0$ and $1.0$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSip1lfHBKaT"
      },
      "source": [
        "def eval_classifier(learner, data_train, labels_train, data_test, labels_test):\n",
        "    (th, th0) = learner(data_train, labels_train, params={})\n",
        "    total_score = score(data_test, labels_test, th, th0)\n",
        "    (num_rows, num_cols) = data_test.shape\n",
        "    return total_score / num_cols"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beHMGAb6BTu1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a0a2668-90b6-41a1-d115-d064ca42ee94"
      },
      "source": [
        "test_eval_classifier(eval_classifier, perceptron)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------Test Eval Classifier 0-----------\n",
            "Passed! \n",
            "\n",
            "-----------Test Eval Classifier 1-----------\n",
            "Passed! \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WPStky3GiJb"
      },
      "source": [
        "### 6.2) Evaluating a Learning algorithm using a data source\n",
        "\n",
        "Construct a testing procedure that takes a learning algorithm and a data source as input and runs the learning algorithm multiple times, each time evaluating the resulting classifier as above. It should report the overall average classification accuracy.\n",
        "\n",
        "You can use our implementation of `eval_classifier` by viewing the answer for that question and copying its definition into your code cell.\n",
        "\n",
        "Write the function `eval_learning_alg` that takes:\n",
        "\n",
        "* `learner` - a function, such as `perceptron` or `averaged_perceptron`\n",
        "* `data_gen` - a data generator, call it with a desired data set size; returns a tuple `(data, labels)`\n",
        "* `n_train` - the size of the learning sets\n",
        "* `n_test` - the size of the test sets\n",
        "* `it` - the number of iterations to average over\n",
        "\n",
        "and returns the average classification accuracy as a float between $0.0$ and $1.0$.\n",
        "\n",
        "**Note: Be sure to generate your training data separately before testing data, to ensure that the pseudo-randomly generated data matches that in the test code.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qytb8giBXZq"
      },
      "source": [
        "def eval_classifier(learner, data_train, labels_train, data_test, labels_test):\n",
        "    (th, th0) = learner(data_train, labels_train, params={})\n",
        "    total_score = score(data_test, labels_test, th, th0)\n",
        "    (num_rows, num_cols) = data_test.shape\n",
        "    return total_score / num_cols\n",
        "\n",
        "def eval_learning_alg(learner, data_gen, n_train, n_test, it):\n",
        "    score = 0.0\n",
        "    for i1 in range(it):\n",
        "      (data_train, labels_train) = data_gen(n_train) \n",
        "      (data_test, labels_test) = data_gen(n_test)\n",
        "      score = score + eval_classifier(learner, data_train, labels_train, data_test, labels_test)  \n",
        "    return score/it"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCZojUBJBb06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b999c0e2-9050-4399-8c06-4caf47c0a694"
      },
      "source": [
        "test_eval_learning_alg(eval_learning_alg, perceptron)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------Test Eval Learning Algo-----------\n",
            "Passed! \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60u9G0QnGzv-"
      },
      "source": [
        "## 6.3) Evaluating a Learning Algorithm With a Fixed Dataset\n",
        "\n",
        "Cross-validation is a strategy for evaluating a learning algorithm, using a single training set of size $n$. Cross-validation takes in a learning algorithm $L$, a fixed data set $\\mathcal{D}$, and a parameter $k$. It will run the learning algorithm $k$ different times, then evaluate the accuracy of the resulting classifier, and ultimately return the average of the accuracies over each of the $k$ \"runs\" of $L$. It is structured like this:\n",
        "\n",
        "<pre><code>divide D into k parts, as equally as possible;  call them D_i for i == 0 .. k-1\n",
        "# be sure the data is shuffled in case someone put all the positive examples first in the data!\n",
        "for j from 0 to k-1:\n",
        "    D_minus_j = union of all the datasets D_i, except for D_j\n",
        "    h_j = L(D_minus_j)\n",
        "    score_j = accuracy of h_j measured on D_j\n",
        "return average(score0, ..., score(k-1))</code></pre>\n",
        "\n",
        "So, each time, it trains on  $k−1$ of the pieces of the data set and tests the resulting hypothesis on the piece that was not used for training.\n",
        "\n",
        "When $k=n$, it is called *leave-one-out cross validation*.\n",
        "\n",
        "Implement cross validation **assuming that the input data is shuffled already** so that the positives and negatives are distributed randomly. If the size of the data does not evenly divide by k, split the data into `n % k` sub-arrays of size `n // k + 1` and the rest of size `n // k`.\n",
        "\n",
        "You can use <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.array_split.html\">np.array_split</a>\n",
        "and <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.html\">np.concatenate</a> with axis arguments to split and rejoin the data as you desire. You can also use our implementation of `eval_classifier` by viewing the answer for that question and copying its definition into your code cell.\n",
        "\n",
        "Note: In Python, `n//k` indicates integer division, e.g. `2//3 = 0` and `4//3 = 1`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5_iixOmBgR7"
      },
      "source": [
        "def xval_learning_alg(learner, data, labels, k):\n",
        "    # Cross validation of learning algorithm\n",
        "    # Your implementation here\n",
        "    (d, n) = data.shape\n",
        "    n_section = n // k    # Size of each section\n",
        "    data_new = np.array_split(data, k, axis=1)\n",
        "    labels_new = np.array_split(labels, k, axis=1)\n",
        "    score = np.zeros((k,1))\n",
        "   \n",
        "    for i1 in range(k):\n",
        "      data_test = data_new[i1]      # Test data\n",
        "      labels_test = labels_new[i1]  # Test data\n",
        "      init = True\n",
        "      \n",
        "      for i2 in range(k):\n",
        "        if i2 != i1:                # Exclusion of test data\n",
        "          if init == True:\n",
        "            data_train = data_new[i2]   # Initialise\n",
        "            labels_train = labels_new[i2]\n",
        "            init = False\n",
        "          else:  \n",
        "            data_train = np.concatenate((data_train, data_new[i2]), axis=1)\n",
        "            labels_train = np.concatenate((labels_train, labels_new[i2]), axis=1)\n",
        "\n",
        "      score[i1] = eval_classifier(learner, data_train, labels_train, data_test, labels_test)\n",
        "    \n",
        "    return np.mean(score)\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "   \n",
        "  \n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUiUgtMHBiZX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ec3628e-3843-4d45-8bbc-956ac5af3347"
      },
      "source": [
        "test_xval_learning_alg(xval_learning_alg, perceptron)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------Test Cross-eval Learning Algo-----------\n",
            "Passed! \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRpz2O3LjHPw"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}